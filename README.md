# 爬虫项目集合

## 01 天善智能课程爬虫

### 需求分析

requests实现爬取天善智能所有课程信息. 反爬措施非常弱, 可以不用考虑反爬.

### 难点

课程是从 "课程形式", "课程方向"和"课程标签" 三个不同的方面进行分类的, 如何获取某个课程对应的这三个分类信息. 解决方法, 设置三个以大分类名为名称的字段, 分别保存这三个大分类的信息. 对三个大分类中每个小分类进行遍历, 记录下每个小分类的课程详情url地址, 同时把此课程对应的分类字段的值设置为小分类的名称.

## 02  在线视频下载

### 需求

国外sci_exp学习网站上有大量的理工科学习资源, 但是需要付费才能观看和下载, 并且付费价格不是一般的贵. 使用selenium+chrome自动下载网站视频.

## 实现功能:

视频重命名为 ”上传日期_标题_视频id_原始视频名称” 的形式, 保存到上传年份为名的文件夹中, 视频缩略图的下载, 视频基本信息的提取. 写入视频信息到本地与视频同名的txt文件中, 保存视频信息到mysql数据库中.

### 难点

1. 网站的登录和认证方式不同于其它的网站, 无法使用requests的session模拟登录. 如果使用requests, 则必须要在每个requests中都加入HTTPBasicAuth进行认证. 使用selenium+chrome却可以绕过这个问题.
2. 网站的反爬机制, 使用requests库下载视频时大约10个小时就会被网站禁止一次. 但使用selenium+chrome就能完全模拟浏览器的行为, 从而绕过网站的反爬机制.
3. 想要实现在视频下载链接上点击右键从右键菜单中选择"另存为", 然后重命名保存的视频名称和地址, 再点击保存. 但selenium无法操作鼠标右键和windows对话框, 使用pywin32模块和AutoIT来操作鼠标右键及windows对话框, 选择"另存为", 及视频的重命名.
4. 网站被国内屏蔽, 需要使用国外代理地址才能正常访问和下载, 这里使用shadowsocks客户端实现代理.



## 03 豆瓣图书爬虫

### 需求分析

爬取豆瓣图书所有图书信息并保存到json文件中.

### 难点

页面结束页的确认. 页面显示的图书数量与实际的图书数量不符合, 只能从页面本身的结构特点进行判断, 如果页面中提取不到图书信息, 就认为图书列表页已到达最后一页.


## 04 博学谷所有已报课程信息爬虫

### 需求分析

爬取博学谷所报的所有课程的详情信息, 并写入到txt文件中.

### 难点

无压力, 只有一些小的细节需要注意. 
如微博登录时需要手动输入验证码
如果出现促销信息, 需要先关闭才能进行登录
写入到文件时课程名中可能存在着 '/'
在 "就业班", "微课" 中提取的课程信息可能会出现重复的
无原因的提取到课程url为None的信息


## 05 博学谷所有就业班课程信息爬虫

### 需求分析

爬取博学谷所有就业班课程的详情信息, 并写入到txt文件中.

博学谷所有就业班课程的url地址格式如下, 可以使用遍历从0开始尝试, 如果地址存在, 就把能够 "免费试学" 的章节的详细章节信息提取出来.
https://xuexi.boxuegu.com/class_track.html?courseId=1132&isFree=1
经过遍历, 得到有效的课程id列表为

[70, 76, 123, 135, 218, 220, 222, 237, 244, 274, 293, 309, 322, 328, 329, 435, 436, 469, 470, 486, 555, 587, 795, 802, 907, 909, 944, 954, 955, 956, 957, 958, 959, 969, 979, 981, 992, 994, 1017, 1022, 1047, 1083, 1092, 1109, 1110, 1111, 1112, 1113, 1114, 1120, 1121, 1123, 1125, 1126, 1127, 1128, 1129, 1131, 1132, 1133, 1136, 1146, 1168, 1169, 1170, 1173, 1180, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1198, 1199, 1200, 1222, 1230, 1234, 1258, 1259, 1276]

### 难点

只能提取到能够 "免费试学" 章节的详情信息, 其它的无法提取.